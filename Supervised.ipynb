{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279001ba",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c1afa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "897de93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "positive = 'train/pos'\n",
    "negative = 'train/neg'\n",
    "\n",
    "X_text = []\n",
    "y = []\n",
    "\n",
    "# Load positive reviews\n",
    "for filename in os.listdir(positive):\n",
    "    filepath = os.path.join(positive, filename)\n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        X_text.append(file.read())\n",
    "        y.append(1)\n",
    "\n",
    "# Load negative reviews\n",
    "for filename in os.listdir(negative):\n",
    "    filepath = os.path.join(negative, filename)\n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        X_text.append(file.read())\n",
    "        y.append(0)\n",
    "\n",
    "print(f\"Loaded {len(X_text)} reviews.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813bc30",
   "metadata": {},
   "source": [
    "Vectorize the text into TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94375520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize raw text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vec = vectorizer.fit_transform(X_text)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_vec.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "494214cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (20000, 5000)\n",
      "Validation set shape: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "#split the data into training and validation sets 80/20\n",
    "X_train, X_val, y_train_split, y_val_split = train_test_split(\n",
    "    X_vec, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60aae1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA-reduced train shape: (20000, 100)\n",
      "PCA-reduced val shape: (5000, 100)\n"
     ]
    }
   ],
   "source": [
    "#Define Feature Transformations\n",
    "# Apply PCA to reduce TF-IDF dimensions\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train.toarray())\n",
    "X_val_pca = pca.transform(X_val.toarray())\n",
    "\n",
    "print(f\"PCA-reduced train shape: {X_train_pca.shape}\")\n",
    "print(f\"PCA-reduced val shape: {X_val_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4e5db19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial-expanded train shape: (20000, 5051)\n",
      "Polynomial-expanded val shape: (5000, 5051)\n"
     ]
    }
   ],
   "source": [
    "# Expand PCA features with Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_train_poly = poly.fit_transform(X_train_pca)\n",
    "X_val_poly = poly.transform(X_val_pca)\n",
    "\n",
    "print(f\"Polynomial-expanded train shape: {X_train_poly.shape}\")\n",
    "print(f\"Polynomial-expanded val shape: {X_val_poly.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb23e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR DISPLAYING GRAPH FOR EACH MODEL LATER ON\n",
    "def plot_performance(df_results, param_name, model_name, transformation_name):\n",
    "    plt.plot(df_results[param_name], df_results['Validation Accuracy'], marker='o')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title(f'{model_name} ({transformation_name})')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995481f4",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION WITH 3 TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8030e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
